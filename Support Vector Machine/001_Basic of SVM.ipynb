{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNBqX1wjHQrXBEQ/hlKszwb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Support Vector Machine (SVM) is a powerful supervised learning algorithm used for both classification and regression tasks, though it’s more commonly used for classification. SVM works by finding the hyperplane that best divides a dataset into classes."],"metadata":{"id":"UNu5DkVKlrRv"}},{"cell_type":"markdown","source":["##What is SVM?\n","SVM is a discriminative classifier formally defined by a separating hyperplane.\n","\n","For a 2D space, this hyperplane is a line dividing the data points into two classes. In higher dimensions, it becomes a hyperplane. The key goal of SVM is to find the maximum-margin hyperplane that separates the two classes."],"metadata":{"id":"EVcaIkY5l01X"}},{"cell_type":"markdown","source":["##How Does SVM Work?\n","\n","\n","###a) Separation of Classes:\n"," - The algorithm looks for the hyperplane that maximally separates the data into different classes. This is called the decision boundary.\n"," - In 2D, the decision boundary is a line, while in 3D or higher dimensions, it is a hyperplane.\n","###b) Maximum Margin:\n"," - Margin is the distance between the hyperplane and the closest data points (these points are called support vectors). SVM tries to maximize this margin.\n","###c) Support Vectors:\n"," -These are the data points that are closest to the hyperplane and influence its position. The optimal hyperplane is chosen by using these support vectors, and removing the non-support vectors won’t change the result.\n","###d) Kernel Trick:\n"," -In cases where the data isn’t linearly separable, SVM uses a technique called the kernel trick to project the data into a higher-dimensional space where it can become separable.\n"," -Common kernels include:\n","  - Linear kernel: Works when the data is linearly separable.\n","  - Polynomial kernel: Good for non-linear data with interaction between features.\n","  - Radial Basis Function (RBF) kernel: Works well for most general non-linear cases."],"metadata":{"id":"q4vwfFsAl032"}},{"cell_type":"markdown","source":["##Advantages of SVM\n"," - Effective in high-dimensional spaces: SVM works well in scenarios where the number of features is greater than the number of samples.\n"," -Memory Efficient: Only a subset of the training points (support vectors) is used to define the hyperplane.\n"," - Versatile: SVM can be adapted for non-linear data by using kernel functions.\n"," - Outlier Resistant: By focusing on support vectors, SVM can be less sensitive to outliers compared to other algorithms like logistic regression.\n"],"metadata":{"id":"NdKX2AeFnK0H"}},{"cell_type":"markdown","source":["##Disadvantages of SVM\n"," - Not suitable for large datasets: Training time can be significant for large datasets.\n"," - Less effective when classes are overlapping: If the data has considerable overlap between classes, SVM may not perform well.\n"," - Choosing the right kernel: Selecting the appropriate kernel and its parameters can be complex.\n"],"metadata":{"id":"rGhbr3-MnLKH"}}]}